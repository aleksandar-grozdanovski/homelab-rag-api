# Homelab RAG API

A **Retrieval-Augmented Generation (RAG)** API built with .NET 10 that allows you to chat with your homelab documentation using AI.

## üéØ What It Does

- **Ingests** markdown documentation from your ObsidianVault
- **Generates embeddings** using Ollama (Llama 3.2)
- **Stores** 3072-dimensional vectors in PostgreSQL with pgvector
- **Answers questions** based on your documentation with source citations

## üèóÔ∏è Architecture

```
User Question ‚Üí Embedding (Ollama) ‚Üí Vector Search (PostgreSQL) 
‚Üí Retrieve Top-K Similar Chunks ‚Üí Send to LLM with Context 
‚Üí AI-Generated Answer + Sources
```

## üõ†Ô∏è Tech Stack

- **.NET 10** (LTS) - Web API
- **PostgreSQL 17** with **pgvector** - Vector database
- **Ollama** (Llama 3.2) - Local LLM for embeddings & text generation
- **Entity Framework Core 10** - ORM
- **Docker Compose** - Local development

## üìã Prerequisites

- .NET 10 SDK
- Docker & Docker Compose
- Ollama running with Llama 3.2 model
  ```bash
  ollama pull llama3.2
  ```

## üöÄ Quick Start

### 1. Start PostgreSQL

```bash
docker compose up -d
```

### 2. Apply Database Migrations

```bash
cd HomelabRAG.API
dotnet ef database update
```

### 3. Run the API

```bash
dotnet run --urls="http://localhost:5000"
```

### 4. Verify Health

```bash
curl http://localhost:5000/healthz
# Response: {"status":"healthy","timestamp":"..."}
```

## üìö API Endpoints

### Health Check
```bash
GET /healthz
```

### Ingest Single Document
```bash
POST /api/documents/ingest
Content-Type: application/json

{
  "filePath": "/path/to/document.md"
}
```

### Ingest Directory (Batch)
```bash
POST /api/documents/ingest-directory
Content-Type: application/json

{
  "directoryPath": "/path/to/markdown/files"
}
```

### List Documents
```bash
GET /api/documents
```

### Query (Ask Questions)
```bash
POST /api/query
Content-Type: application/json

{
  "question": "How do I deploy to Kubernetes?",
  "topK": 5  // Optional: number of similar chunks to retrieve (default: 5)
}
```

## üí° Usage Examples

### Ingest Your Documentation
```bash
curl -X POST http://localhost:5000/api/documents/ingest-directory \
  -H "Content-Type: application/json" \
  -d '{"directoryPath": "/home/user/ObsidianVault/02-Knowledge"}'
```

### Ask a Question
```bash
curl -X POST http://localhost:5000/api/query \
  -H "Content-Type: application/json" \
  -d '{"question": "How do I install Flux CD?"}'
```

**Example Response:**
```json
{
  "question": "How do I install Flux CD?",
  "answer": "Based on the provided documentation, you can install Flux CD by running: flux install --version 2.7.5",
  "sources": [
    {
      "fileName": "GitOps-Flux-Setup.md",
      "chunkIndex": 0,
      "preview": "# GitOps with Flux CD - Setup Guide..."
    }
  ],
  "chunksUsed": 5
}
```

## üîß Configuration

Edit `appsettings.json`:

```json
{
  "ConnectionStrings": {
    "DefaultConnection": "Host=localhost;Port=5432;Database=homelab_rag;Username=postgres;Password=postgres"
  },
  "Ollama": {
    "BaseUrl": "http://192.168.50.10:11434",
    "Model": "llama3.2"
  }
}
```

## üìä Database Schema

### Documents
- `Id` - Primary key
- `FileName` - Document filename
- `FilePath` - Full path to source file
- `Content` - Full document content
- `IngestedAt` - Timestamp

### DocumentChunks
- `Id` - Primary key
- `DocumentId` - Foreign key to Documents
- `Content` - Text chunk (max ~1000 chars)
- `ChunkIndex` - Position in document
- `Embedding` - 3072-dimensional vector (Llama 3.2)

## üéì How It Works

1. **Document Ingestion**
   - Reads markdown files
   - Splits into ~1000 character chunks (paragraph boundaries)
   - Generates embeddings via Ollama API
   - Stores in PostgreSQL with pgvector

2. **Query Processing**
   - Converts question to embedding
   - Performs cosine similarity search
   - Retrieves top-K most similar chunks
   - Sends chunks + question to Llama 3.2
   - Returns AI-generated answer with sources

3. **Vector Search**
   - Uses pgvector extension for efficient similarity search
   - Cosine distance metric
   - Indexed for performance

## üöÄ Deployment to Kubernetes

*(Coming soon: Kubernetes manifests, GitOps with Flux)*

## üìù Interview Talking Points

- "Built a production RAG system with .NET 10 and PostgreSQL pgvector"
- "Integrated local LLM (Llama 3.2) for embeddings and text generation"
- "Implemented vector similarity search for semantic document retrieval"
- "Chunking strategy optimized for 3072-dimensional embeddings"
- "Real-world application: Chat with homelab documentation"

## üêõ Troubleshooting

### Port Already in Use
```bash
pkill -f "dotnet run"
```

### Database Connection Issues
```bash
docker compose ps  # Verify PostgreSQL is running
docker compose logs postgres
```

### Ollama Not Reachable
```bash
curl http://192.168.50.10:11434/api/tags  # Test Ollama API
```

### Vector Dimension Mismatch
Ensure `RAGDbContext.cs` has correct dimension:
```csharp
.HasColumnType("vector(3072)")  // Match Llama 3.2 embedding size
```

## üìñ Resources

- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Ollama API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [RAG Fundamentals](https://www.pinecone.io/learn/retrieval-augmented-generation/)

## üéØ Future Enhancements

- [ ] Add streaming responses
- [ ] Implement conversation memory
- [ ] Add document re-ranking
- [ ] Deploy to K3s cluster
- [ ] GitOps with Flux CD
- [ ] Prometheus metrics
- [ ] Simple web UI
- [ ] Support for PDF documents
- [ ] Multi-model support (switch between Llama/Phi-3)

## üìÑ License

MIT

## üë®‚Äçüíª Author

Built as part of Berlin Platform Engineer interview preparation - Week 8 AI Integration
